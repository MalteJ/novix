global:
  scrape_interval: 1m
  evaluation_interval: 1m

alerting:
  alertmanagers:
    - static_configs:
      - targets:
        # - alertmanager:9093

# Load rules once and periodically evaluate them
rule_files:
  # - "rules.yml"

scrape_configs:
  # first we scrape ourselves
  - job_name: "prometheus"
    # path defaults to '/metrics'
    # scheme defaults to 'http'

    static_configs:
      - targets: ["alice.nivenly.com:9090"]
      - targets: ["esme.nivenly.com:9090"]

  # get some handy metrics from the node exporter
  - job_name: "node"
    static_configs:
      - targets: ["alice.nivenly.com:9100"]
      - targets: ["esme.nivenly.com:9100"]

  # do some blackbox probing to make sure we know when
  # services are up and stuff
  - job_name: "blackbox"
    metrics_path: /probe
    params:
      module: [http_2xx_get]
    static_configs:
      - targets:
        - https://hachyderm.io
    relabel_configs:
      - source_labels: [__address__]
        target_label: __param_target
      - source_labels: [__param_target]
        target_label: instance
      - target_label: __address__
        replacement: localhost:9115

  # TODO: add this once the basics are working to get mastodon
  # metrics
  #- job_name: "statsd"
    # TODO
